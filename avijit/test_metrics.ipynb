{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24eb6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysteps as py\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79dac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir ='/aspire/CarloData/zz_UNETs/Avijit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9cefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368, 368, 5)\n",
      "(368, 368, 5)\n"
     ]
    }
   ],
   "source": [
    "result_dir = os.path.join(main_dir, 'RESULTS') \n",
    "region_dir = os.path.join(result_dir, 'CentralEurope') # change for different regions [CentralEurope, Iberia, Scandinavia]\n",
    "model_dir = os.path.join(region_dir, 'diffusion') # change for different models [diffusion, UNetPSD, UNetVanilla]\n",
    "gt_dir = os.path.join(region_dir, 'Target') # Ground truth \n",
    "pred_path = os.path.join(model_dir, 'nwp_2021020710.npy') # change for different dates\n",
    "pred_data = np.load(pred_path)\n",
    "print(pred_data.shape)\n",
    "gt_path = os.path.join(gt_dir, 'nwp_2021020110.npy') # change for different dates\n",
    "gt_data = np.load(gt_path)\n",
    "print(gt_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a57280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nvtx\n",
    "import torch\n",
    "import os\n",
    "from scipy.stats import ks_2samp  \n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def compute_metrics(\n",
    "    path_gt: str,\n",
    "    path_pred: str,\n",
    "    save_dir: str,\n",
    "    *,\n",
    "    var_names: list[str] | None = None,\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compare every .npy file that exists in BOTH `path_gt` and `path_pred`,\n",
    "    assuming each file has shape (H, W, C) with the same C variables.\n",
    "\n",
    "    Returns a nested dict: metrics[var_name][metric] = value\n",
    "    and writes the same information to <save_dir>/metrics_new.txt.\n",
    "    \"\"\"\n",
    "\n",
    "    path_gt, path_pred = Path(path_gt), Path(path_pred)\n",
    "    gt_files   = {f.name: f for f in path_gt.glob(\"*.npy\")}\n",
    "    pred_files = {f.name: f for f in path_pred.glob(\"*.npy\")}\n",
    "    common     = sorted(gt_files.keys() & pred_files.keys())\n",
    "\n",
    "    if not common:\n",
    "        raise FileNotFoundError(\"No overlapping .npy filenames in the two folders.\")\n",
    "\n",
    "    # ----- discover channel count from the first file\n",
    "    first = np.load(gt_files[common[0]])\n",
    "    if first.ndim != 3:\n",
    "        raise ValueError(\n",
    "            f\"Expected shape (H, W, C). Found {first.shape} in {common[0]!r}\"\n",
    "        )\n",
    "    C = first.shape[-1]\n",
    "    if var_names is None:\n",
    "        var_names = [f\"var{c}\" for c in range(C)]\n",
    "    # if len(var_names) != C:\n",
    "    #     raise ValueError(\"Length of var_names must equal number of channels (C).\")\n",
    "\n",
    "    # accumulators: metric_sums[var][metric] = running total\n",
    "    metric_template = {\n",
    "        \"MAE\": 0.0,\n",
    "        \"RMSE\": 0.0,\n",
    "        \"SSIM\": 0.0,\n",
    "        \"PSNR\": 0.0,\n",
    "        \"Cramer\": 0.0,\n",
    "        \"KS\": 0.0,\n",
    "        \"Hill\": 0.0,\n",
    "    }\n",
    "    metric_sums = {v: metric_template.copy() for v in var_names}\n",
    "\n",
    "    # global metrics that combine u and v (channels 0 and 1)\n",
    "    global_sums: dict[str, float] = {}\n",
    "\n",
    "    n_files = 0\n",
    "\n",
    "    # ── per‑file loop ─────────────────────────────────────────────────────────\n",
    "    for fname in common:\n",
    "        gt  = np.load(gt_files[fname]).astype(np.float32)\n",
    "        prd = np.load(pred_files[fname]).astype(np.float32)\n",
    "\n",
    "        gt  = ensure_channels_last(gt,  C)\n",
    "        prd = ensure_channels_last(prd, C)\n",
    "        \n",
    "        gt_wind_u = gt[..., 0]  # u-component of wind\n",
    "        gt_wind_v = gt[..., 1]\n",
    "        prd_wind_u = prd[..., 0]\n",
    "        prd_wind_v = prd[..., 1]\n",
    "        wind_speed_prd, wind_speed_gt = compute_wind_speed(prd_wind_u, prd_wind_v, gt_wind_u, gt_wind_v)\n",
    "        vorticity_prd, vorticity_gt = compute_vorticity(prd_wind_u, prd_wind_v, gt_wind_u, gt_wind_v)\n",
    "        gt = np.concatenate(\n",
    "            [gt, wind_speed_gt[..., None], vorticity_gt[..., None]], axis=-1\n",
    "        )\n",
    "        prd = np.concatenate(\n",
    "            [prd, wind_speed_prd[..., None], vorticity_prd[..., None]], axis=-1\n",
    "        )\n",
    "\n",
    "        # ── per‑variable metrics ────────────────────────────────────────────\n",
    "        for c, vname in enumerate(var_names):\n",
    "            g = gt[..., c]\n",
    "            p = prd[..., c]\n",
    "\n",
    "            metric_sums[vname][\"MAE\"]    += compute_mae(p, g)\n",
    "            metric_sums[vname][\"RMSE\"]   += compute_rmse(p, g)\n",
    "            metric_sums[vname][\"SSIM\"]   += compute_ssim_metric(p, g)\n",
    "            metric_sums[vname][\"PSNR\"]   += compute_psnr_metric(p, g)\n",
    "            metric_sums[vname][\"Cramer\"] += 0 #compute_cramer(p, g)\n",
    "            metric_sums[vname][\"KS\"]     += compute_ks_metric(p, g)\n",
    "            metric_sums[vname][\"Hill\"]   += compute_hill_metric(p, g)\n",
    "\n",
    "        n_files += 1\n",
    "\n",
    "    # ── average across files ─────────────────────────────────────────────────\n",
    "    metrics = {\n",
    "        v: {m: total / n_files for m, total in metric_sums[v].items()}\n",
    "        for v in var_names\n",
    "    }\n",
    "    if global_sums:\n",
    "        metrics[\"_GLOBAL_\"] = {m: total / n_files for m, total in global_sums.items()}\n",
    "\n",
    "    # ── save to disk ─────────────────────────────────────────────────────────\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    out_path = Path(save_dir) / \"metrics_new.txt\"\n",
    "    with open(out_path, \"w\") as fh:\n",
    "        for v in list(metrics.keys()):\n",
    "            fh.write(f\"[{v}]\\n\")\n",
    "            for m, val in metrics[v].items():\n",
    "                fh.write(f\"  {m}: {val:.6f}\\n\")\n",
    "            fh.write(\"\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ─────────────────────── basic metrics helpers ───────────────────────────────\n",
    "def compute_mae(p: np.ndarray, g: np.ndarray) -> float:\n",
    "    return np.abs(p - g).mean()\n",
    "\n",
    "\n",
    "def compute_rmse(p: np.ndarray, g: np.ndarray) -> float:\n",
    "    return np.sqrt(np.mean((p - g) ** 2))\n",
    "\n",
    "\n",
    "def compute_ssim_metric(p: np.ndarray, g: np.ndarray) -> float:\n",
    "    dr = g.max() - g.min()\n",
    "    return float(ssim(g, p, data_range=dr))\n",
    "\n",
    "\n",
    "def compute_psnr_metric(p: np.ndarray, g: np.ndarray) -> float:\n",
    "    dr = g.max() - g.min()\n",
    "    return float(psnr(g, p, data_range=dr))\n",
    "\n",
    "\n",
    "def _mean_pairwise_abs(sorted_v: np.ndarray) -> float:\n",
    "    n      = sorted_v.size\n",
    "    coeffs = 2 * np.arange(n) - n + 1        # 0‑based version of 2i−n−1\n",
    "    return (2.0 / n**2) * coeffs.dot(sorted_v)\n",
    "\n",
    "def _mean_cross_abs(sorted_x: np.ndarray, sorted_y: np.ndarray) -> float:\n",
    "    n, m   = sorted_x.size, sorted_y.size\n",
    "    cumsum = np.concatenate(([0.0], np.cumsum(sorted_y)))\n",
    "    idx    = np.searchsorted(sorted_y, sorted_x, side=\"left\")\n",
    "\n",
    "    left  = sorted_x * idx             - cumsum[idx]\n",
    "    right = (cumsum[-1] - cumsum[idx]) - sorted_x * (m - idx)\n",
    "    return (left + right).sum() / (n * m)\n",
    "\n",
    "def compute_cramer(p: np.ndarray, g: np.ndarray) -> float:\n",
    "    p_sorted = np.sort(p.ravel())\n",
    "    g_sorted = np.sort(g.ravel())\n",
    "\n",
    "    dxy = _mean_cross_abs(g_sorted, p_sorted)\n",
    "    dgg = _mean_pairwise_abs(g_sorted)\n",
    "    dpp = _mean_pairwise_abs(p_sorted)\n",
    "    return 2.0 * dxy - dgg - dpp\n",
    "\n",
    "\n",
    "# ───────────────────── physics‑oriented helpers ──────────────────────────────\n",
    "def compute_wind_speed(u_p: np.ndarray, v_p: np.ndarray,\n",
    "                      u_g: np.ndarray, v_g: np.ndarray) -> float:\n",
    "    \"\"\"RMSE of wind‑speed magnitude.\"\"\"\n",
    "    speed_p = np.hypot(u_p, v_p)\n",
    "    speed_g = np.hypot(u_g, v_g)\n",
    "    return speed_p, speed_g\n",
    "\n",
    "\n",
    "def compute_vorticity(u_p: np.ndarray, v_p: np.ndarray,\n",
    "                           u_g: np.ndarray, v_g: np.ndarray,\n",
    "                           dx: float = 5500.0, dy: float = 5500.0) -> float:\n",
    "    \"\"\"\n",
    "    RMS of vorticity error ζ = ∂v/∂x − ∂u/∂y using centred finite differences.\n",
    "    \"\"\"\n",
    "    ζ_p = np.gradient(v_p, dx, axis=1) - np.gradient(u_p, dy, axis=0)\n",
    "    ζ_g = np.gradient(v_g, dx, axis=1) - np.gradient(u_g, dy, axis=0)\n",
    "    return ζ_g, ζ_p\n",
    "\n",
    "\n",
    "def compute_ks_metric(p: np.ndarray, g: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Two‑sample Kolmogorov–Smirnov statistic (two‑sided).\n",
    "    \"\"\"\n",
    "    return ks_2samp(p.ravel(), g.ravel(), alternative=\"two-sided\").statistic\n",
    "\n",
    "\n",
    "def compute_hill_metric(p: np.ndarray, g: np.ndarray, k: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Absolute difference of Hill tail indices – focuses on heavy‑tail behaviour.\n",
    "    \"\"\"\n",
    "    def hill(x: np.ndarray, k_: int) -> float:\n",
    "        x = np.abs(x.ravel()) + 1e-6     # ensure strictly positive\n",
    "        x_sorted = np.sort(x)[::-1]      # descending\n",
    "        k_ = min(k_, len(x_sorted) - 1)\n",
    "        x_k = x_sorted[k_]\n",
    "        return (1.0 / k_) * np.log(x_sorted[:k_] / x_k).sum()\n",
    "\n",
    "    return abs(hill(p, k) - hill(g, k))\n",
    "\n",
    "\n",
    "# ─────────────────────────── utility ─────────────────────────────────────────\n",
    "def ensure_channels_last(arr: np.ndarray, C_expected: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert array to (H, W, C) format if currently (C, H, W).\n",
    "    Raises if channel axis cannot be found.\n",
    "    \"\"\"\n",
    "    if arr.shape[-1] == C_expected:      # already (H, W, C)\n",
    "        return arr\n",
    "    if arr.shape[0] == C_expected:       # assume (C, H, W)\n",
    "        return np.moveaxis(arr, 0, -1)   # -> (H, W, C)\n",
    "    raise ValueError(f\"Cannot locate channel axis in shape {arr.shape}\")\n",
    "\n",
    "\n",
    "# ──────────────────────────── CLI ────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Compute metrics for two folders of (H,W,C) .npy files.\"\n",
    "    )\n",
    "    parser.add_argument(\"--path_gt\", help=\"Directory with ground‑truth .npy files\")\n",
    "    parser.add_argument(\"--path_pred\", help=\"Directory with prediction  .npy files\")\n",
    "    parser.add_argument(\"--save_dir\", help=\"Where metrics_new.txt will be written\")\n",
    "    parser.add_argument(\n",
    "        \"--var_names\",\n",
    "        nargs=\"*\",\n",
    "        default=None,\n",
    "        help=(\"Optional list of variable names (length must equal channel count), \"\n",
    "              \"e.g. --var_names u10 v10 t2m sshf zust\"),\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        args.path_gt,\n",
    "        args.path_pred,\n",
    "        args.save_dir,\n",
    "        var_names=['u10', 'v10', 't2m', 'sshf', 'zust', \"wind_speed\", \"vorticity\"],\n",
    "    )\n",
    "\n",
    "    pprint(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pysteps_eval)",
   "language": "python",
   "name": "pysteps_eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
